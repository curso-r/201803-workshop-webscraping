---
title: "Workshop: Web Scraping em R"
author: "Caio Lente + Fernando Corrêa"
date: "2018-03-10"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<!-- ### GLOBAIS ########################################################## -->

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(comment = "#>")
library(magrittr)
```



<!-- ### ABERTURA ######################################################### -->

# Quem Somos

A _Curso-R_:
- O grupo surgiu em 2015 para ministrar o curso "Programação em R: do
casual ao avançado" no IME-USP
- Em 2016 e 2017, ministramos novamente o curso e, depois de mais de 100 alunos
formados, decidimos aumentar a gama e a frequência das aulas

Caio:
- Bacharelando em Ciência da Computação pelo IME-USP
- Estagiário na Platipus Consultoria
- Metido a designer, maníaco da organização e metade texano

Fernando:
- Bacharel e mestrando em Estatística pelo IME-USP
- Diretor-técnico na Associação Brasileira de Jurimetria
- Nerd, Estatístico e mágico

---
# Prólogo

O termo mais apropriado para tratar de *web scraping* em portugês é "raspagem
web", ou seja, a extração de dados provenientes de uma página online.

Quando se trata de web scraping, a primeira coisa que precisamos fazer é criar
um plano de ação. Raspar dados online não é uma ciência exata, então se não
nos planejarmos com antecedência é bem provável que no final nosso código
fique completamente incompreensível e irreprodutível.

### Expectativa

```
dados <- raspar("site.com")
```

### Realidade

![](static/commits_esaj.png)

---
# Prólogo (cont.)

Por isso recomendamos sempre tentar seguir o **fluxo do web scraping**:

![](static/ciclo.png)

Nós achamos que o fluxo do web scraping é a melhor forma de abordar esse conteúdo
porque, com ele, a tarefa muitas vezes complicada e sem objetivo definido de
raspar dados da web se torna um processo iterativo e bem delimitado que podemos
utilizar nas mais diversas situações.

---
# Conteúdo

Através de exposição, exemplos do mundo real e exercícios veremos cada passo do
fluxo:

- Como **identificar** o objeto de interesse

- Como funciona uma página HTML e como **navegar** por seus elementos

- O protocolo HTTP e como **replicar** seu funcionamento

- Como **parsear** as páginas baixadas

- Como **iterar** ao longo de muitas páginas e como salvar esses resultados

- Como **validar** resultados e evitar os problemas mais comuns do web scraping

- Outras tecnlogias associadas ao web scraping moderno

- Captchas (quando quebrá-los e como quebrá-los)



<!-- ### IDENTIFICAR ###################################################### -->

---
class: inverse, center, middle
# IDENTIFICAR

---
# Quando usar/não-usar web scraping

### Quando usar:

- Quando precisamos coletar um volume grande de dados da internet

### Quando não usar:

- Quando temos uma forma mais simples de obter os dados (API, base de dados, etc.)

- Quando os termos de uso do site não nos permitem fazer isso

- Quando o `robots.txt` do site não nos permite fazer isso

- Quando houver risco de derrubar ou comprometer a estabilidade do site

- Quando as informações do site não são públicas

---
# Encontrar o que você quer

Imagine que você precisa extair alguma informação de um site, seja ela o título
de várias páginas da Wikipédia, os comentários de um post do Reddit ou mesmo
os dados de países contidos no famosíssimo *Example Web Scraping Website*
([http://example.webscraping.com/](http://example.webscraping.com/)).

Se tivermos verificado que de fato não temos nenhuma outra opção e que o site
nos permite raspá-lo, então podemos começar.

O primeiro passo do fluxo consiste em **identificar** os elementos que queremos
extrair e observar se eles se comportam da mesma forma em todas as situações
possíveis.

### Example Web Scraping Website

O nosso site de exemplo será o `http://example.webscraping.com`. Para que
possamos continuar, precisamos salvar manualmente uma página de interesse desse
site (no caso, a home já basta).

---
# Outros casos interessantes

### Extração de textos

Talvez uma das tarefas mais simples de se cumprir em web scraping é extrair
texto bem-estruturado de uma página. Os artigos da Wikipédia são ótimos para
treinar um primeiro scraper, mas eles acabam sendo uma excessão no mundo do
web scraping justamente por serem tão bem estruturados.

Além disso, a Wikipédia possui uma API.

![](static/texto.png)


---
# Outros casos interessantes (cont.)

### Extação de imagens

Baixar imagens já uma tarefa um pouco mais complexa porque elas geralmente estão
menos organizadas que texto e ainda por cima ocupam mais espaço.

A imagem abaixo é do Reddit, que também possui uma API.

![](static/imagem.png)

---
# Outros casos interessantes (cont.)

### Extração de PDFs

Extrair PDFs é um pouco mais complicado do que extrair imagens porque eles
geralmente não estão endereçados na página da mesma forma que o resto do conteúdo.
Além disso, eles são uma das formas mais pesadas de mídia que podemos raspar.

![](static/pdf.png)

---
# Outros casos interessantes (cont.)

### Extação de áudio

Dificilmente temos a necessidade de baixar arquivos de áudio, mas, quando esse é
o caso, o fluxo não muda muito em relação aos PDFs. A dificuldade aqui é em como
tratar esses arquivos porque geralmente são escassos os métodos e bibliotecas de
processamento de som.

![](static/audio.png)



<!-- ### NAVEGAR ########################################################## -->

---
class: inverse, center, middle
# NAVEGAR


---
# HTML

### Estrutura básica

![](static/html_exemplo_1.png)

---
# HTML

![](static/html_exemplo_2.png)

---
# HTML

![](static/html_exemplo_3.png)

---
# HTML

![](static/html_exemplo_3.png)
 - tags: head, meta, title, body, h1, h2, p, html.
 - argumentos: charset, style.

---
x HTML (div's, span e outros tags de conteúdo)
x HTML (atributos)
x CSS (estilizador de html)
x CSS (seletores)
o selector gadget
x outra forma de fazer seleção: XPATH
x xml2 para encontrar coisas (salvar arquivo + read_html)
x xml_find(_all)?
x Exemplo 1:
 - Inspect Texto
x Exemplo 2:
 - Inspect Imagem



<!-- ### REPLICAR ######################################################### -->

---
class: inverse, center, middle
# REPLICAR

---
x protocolo HTTP
x requisições
x exemplo de GET
x exemplo de POST
x respostas
   - o que tem na resposta
       - cookies
       - html
       - status
x Network (como que eu vejo as requests acontecendo)
x httr (como fazer requisições em R)
x httr exemplo: GET e POST
x httr o que vem na resposta
x abrir o html pra ver se deu certo



<!-- ### PARSEAR ########################################################## -->

---
class: inverse, center, middle
# PARSEAR

---

x xml2
x extraindo texto
x select tag do texto
x xml_text
x download de imagem
x tidy output
BREAK (mais ou menos por aqui começa um exemplo)
x example site
o exercícios
 1. logar numa conta (fazer o POST)
 2. baixar a página de andorra
 3. extrair os dados de andorra numa tabela



<!-- ### ITERAR ########################################################### -->

---
class: inverse, center, middle
# ITERAR

---
x baixar html
x salvar tidy output
x carefully
x walk    
o exercício
 1. baixe as páginas de todos os países
 IP BLOCK (os caras vão ter que dar ip 1)
TALVEZ OUTRO (OU O PRIMEIRO) BREAK POSSA ACONTECER AQUI



<!-- ### VALIDAR ########################################################## -->

---
class: inverse, center, middle
# VALIDAR

---
x Event validation e view state (asp)
 - TODO: PENSAR EM OUTRO EXEMPLO
x Páginas jsp
 - TODO:  PENSAR EM OUTRO EXEMPLO
x Evitar armadilhas (catpchas, etc)
 - bloqueio de IP
 - Hackear a URL (comentar)



<!-- ### MISCELÂNEA ####################################################### -->

---
class: inverse, center, middle
# MISCELÂNEA

---
x paralelização e distribuição
x Interatividade
x JS (V8, etc)
x Selenium, phantom
x RMouse



<!-- ### CAPTCHAS ######################################################### -->

---
class: inverse, center, middle
# CAPTCHAS

---
x Modelos ad hoc
x lendo a imagem no R (pacote de ler jpeg ou wtv)
x interpretando o conteudo
x filtrando com magick
x tesseract
o exercíco
 1. faça uma conta automaticamente
x Modelos keras
 - TODO: EXPLICA OU NÃO?
x decryptr


